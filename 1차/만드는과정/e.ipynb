{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-1fb667da5a97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(train_inputs, train_labels), (test_inputs, test_labels) = mnist.load_data()\n",
    "\n",
    "print(train_inputs.shape, train_labels.shape, test_inputs.shape, test_labels.shape)  # 얘네는 numpy array임\n",
    "\n",
    "print(test_inputs.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227295</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227296</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227297</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227298</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227299</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227300 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "...    ..\n",
       "227295  0\n",
       "227296  0\n",
       "227297  0\n",
       "227298  0\n",
       "227299  0\n",
       "\n",
       "[227300 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227300, 10) (227300,) (58746, 10) (58746,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_inputs, train_labels = np.array(pd.read_csv('./cover_train.csv')), np.array(pd.read_csv('./cover_train_label.csv'))\n",
    "test_inputs, test_labels = np.array(pd.read_csv('./cover_test.csv')), np.array(pd.read_csv('./cover_test_label.csv'))\n",
    "\n",
    "train_labels, test_labels = train_labels.reshape(-1,), test_labels.reshape(-1,)\n",
    "\n",
    "print(train_inputs.shape, train_labels.shape, test_inputs.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "x = pd.read_csv('./통_x.csv')\n",
    "y = pd.read_csv('./통_y.csv')\n",
    "anomal_index = y.index[y['0']==1]  # 비정상\n",
    "nomal_index = y.index[y['0']==0]   # 정상\n",
    "normal_y = y.iloc[nomal_index]\n",
    "anomal_y = y.iloc[anomal_index]\n",
    "normal_x = x.iloc[nomal_index]\n",
    "anomal_x = x.iloc[anomal_index]\n",
    "#normal_y\n",
    "#anomal_y\n",
    "normal_y.to_csv('./normal_y.csv', index=False)\n",
    "anomal_y.to_csv('./anomal_y.csv', index=False)\n",
    "normal_x.to_csv('./normal_x.csv', index=False)\n",
    "anomal_x.to_csv('./anomal_x.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   824,    827,    830,    832,    833,    835,    837,    841,\n",
       "               842,    844,\n",
       "            ...\n",
       "            174326, 174327, 174328, 174717, 174938, 175162, 176516, 176739,\n",
       "            176981, 177192],\n",
       "           dtype='int64', length=2747)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomal_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('./normal_x.csv')\n",
    "y = pd.read_csv('./normal_y.csv')\n",
    "\n",
    "to_test_x = x.iloc[:56000]\n",
    "to_test_y = y.iloc[:56000]\n",
    "\n",
    "real_x = x.iloc[56000:]\n",
    "real_y = y.iloc[56000:]\n",
    "\n",
    "to_test_x.to_csv('./to_test_x.csv', index=False)\n",
    "to_test_y.to_csv('./to_test_y.csv', index=False)\n",
    "real_x.to_csv('./real_x.csv', index=False)\n",
    "real_y.to_csv('./real_y.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_digits=list([0])\n",
    "train_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_digits=list(range(2))\n",
    "test_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(x, leak=0.2, name='lrelu'):\n",
    "\treturn tf.maximum(x, leak*x)\n",
    "\n",
    "def build_dense(input_vector,unit_no,activation):    \n",
    "    return tf.layers.dense(input_vector,unit_no,activation=activation,\n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            bias_initializer=tf.zeros_initializer())\n",
    "\n",
    "class MLP_VAE:\n",
    "    def __init__(self,input_dim,lat_dim, outliers_fraction):\n",
    "       # input_paras:\n",
    "           # input_dim: input dimension for X\n",
    "           # lat_dim: latent dimension for Z\n",
    "           # outliers_fraction: pre-estimated fraction of outliers in trainning dataset\n",
    "        \n",
    "        self.outliers_fraction = outliers_fraction # for computing the threshold of anomaly score       \n",
    "        self.input_dim = input_dim\n",
    "        self.lat_dim = lat_dim # the lat_dim can exceed input_dim    \n",
    "        \n",
    "        self.input_X = tf.placeholder(tf.float32,shape=[None,self.input_dim],name='source_x')\n",
    "        \n",
    "        self.learning_rate = 0.0005\n",
    "        self.batch_size =  32\n",
    "        # batch_size should be smaller than normal setting for getting\n",
    "        # a relatively lower anomaly-score-threshold\n",
    "        self.train_iter = 3000\n",
    "        self.hidden_units = 128\n",
    "        self._build_VAE()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.pointer = 0\n",
    "        \n",
    "    def _encoder(self):\n",
    "        with tf.variable_scope('encoder',reuse=tf.AUTO_REUSE):\n",
    "            l1 = build_dense(self.input_X,self.hidden_units,activation=lrelu)\n",
    "#            l1 = tf.nn.dropout(l1,0.8)\n",
    "            l2 = build_dense(l1,self.hidden_units,activation=lrelu)\n",
    "#            l2 = tf.nn.dropout(l2,0.8)          \n",
    "            mu = tf.layers.dense(l2,self.lat_dim)\n",
    "            sigma = tf.layers.dense(l2,self.lat_dim,activation=tf.nn.softplus)\n",
    "            sole_z = mu + sigma *  tf.random_normal(tf.shape(mu),0,1,dtype=tf.float32)\n",
    "        return mu,sigma,sole_z\n",
    "        \n",
    "    def _decoder(self,z):\n",
    "        with tf.variable_scope('decoder',reuse=tf.AUTO_REUSE):\n",
    "            l1 = build_dense(z,self.hidden_units,activation=lrelu)\n",
    "#            l1 = tf.nn.dropout(l1,0.8)\n",
    "            l2 = build_dense(l1,self.hidden_units,activation=lrelu)\n",
    "#            l2 = tf.nn.dropout(l2,0.8)\n",
    "            recons_X = tf.layers.dense(l2,self.input_dim)        # 다시 원래 차원으로 복원\n",
    "        return recons_X           # 복원된 x\n",
    "\n",
    "    def _build_VAE(self):\n",
    "        self.mu_z,self.sigma_z,sole_z = self._encoder()\n",
    "        self.recons_X = self._decoder(sole_z)\n",
    "        \n",
    "        with tf.variable_scope('loss'):\n",
    "            KL_divergence = 0.5 * tf.reduce_sum(tf.square(self.mu_z) + tf.square(self.sigma_z) - tf.log(1e-8 + tf.square(self.sigma_z)) - 1, 1)\n",
    "            mse_loss = tf.reduce_sum(tf.square(self.input_X-self.recons_X), 1)          \n",
    "            self.all_loss =  mse_loss  \n",
    "            self.loss = tf.reduce_mean(mse_loss + KL_divergence)\n",
    "            \n",
    "        with tf.variable_scope('train'):            \n",
    "            self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "            \n",
    "\n",
    "    def _fecth_data(self,input_data):        \n",
    "        if (self.pointer+1) * self.batch_size  >= input_data.shape[0]:\n",
    "            return_data = input_data[self.pointer*self.batch_size:,:]\n",
    "            self.pointer = 0\n",
    "        else:\n",
    "            return_data =  input_data[ self.pointer*self.batch_size:(self.pointer+1)*self.batch_size,:]\n",
    "            self.pointer = self.pointer + 1\n",
    "        return return_data\n",
    "\n",
    "    def train(self,train_X):\n",
    "        for index in range(self.train_iter):\n",
    "            this_X = self._fecth_data(train_X)\n",
    "            self.sess.run([self.train_op],feed_dict={ self.input_X: this_X })\n",
    "        self.arrage_recons_loss(train_X)\n",
    "\n",
    "        \n",
    "    def arrage_recons_loss(self,input_data):      # reconsturction error threshold 세팅\n",
    "        all_losses =  self.sess.run(self.all_loss,feed_dict={self.input_X: input_data })\n",
    "        self.test_loss = np.percentile(all_losses,(1-self.outliers_fraction)*100)         # train을 통해 reconst loss 설정\n",
    "                \n",
    "\n",
    "    def test(self,input_data):\n",
    "        return_label = []\n",
    "        print(\"Reconstruction threshold : \", self.test_loss)\n",
    "        for index in range(input_data.shape[0]):    # test 길이인 8957\n",
    "            single_X = input_data[index].reshape(1,-1)\n",
    "            this_loss = self.sess.run(self.loss,feed_dict={ self.input_X: single_X })\n",
    "            if this_loss < self.test_loss:\n",
    "                return_label.append(0)        # 정상\n",
    "            else:\n",
    "                return_label.append(1)        # 비정상\n",
    "        return return_label\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    scaler = MinMaxScaler()\n",
    "    train = pd.read_csv('data/wbc_train.csv') \n",
    "    test = pd.read_csv('data/wbc_test.csv')                            # csv로 받도록 바꿔주고\n",
    "    test_label = pd.read_csv('data/wbc_test_label.csv')                # numpy일때는 그냥 넘겨줬지만,\n",
    "\n",
    "    train = scaler.fit_transform(train)\n",
    "    test = scaler.transform(test)\n",
    "    #print(train.shape, train.values.shape, test.shape, test.values.shape, test_label.shape, test_label.values.shape)\n",
    "    mlp_vae_predict(pd.DataFrame(train).values, pd.DataFrame(test).values, test_label.values)    # csv일때는 .values로 npy처럼 값으로 넘겨줘서 에러 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,), (1,)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "digits=list(range(2))\n",
    "\n",
    "tasks_to_add = list(permutations(digits, 1))\n",
    "\n",
    "tasks_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아님\n"
     ]
    }
   ],
   "source": [
    "itr = 50\n",
    "\n",
    "if not (itr % 50) and itr > 0:\n",
    "    print(\"아님\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.constant([[1., 1.], [2., 2.]])\n",
    "why = tf.reduce_mean(x)  # 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6c1123bfc613396f7a5d16b167f9758d10a40f855160efe6617310612cad20b"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('pro': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
